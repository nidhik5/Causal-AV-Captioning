# Causal-AV-Captioning

Causal Audio-Visual Video CaptioningThis project implements a framework to evaluate the causal influence of audio on video-language models. The goal is to determine if a model is truly integrating audio information or simply relying on visual bias to generate captions.üìå Project OverviewTraditional Video Language Models (VLMs) often ignore audio cues, leading to "Visual Bias." This project uses Causal Inference to quantify how much the audio stream actually "causes" the model to generate specific words. We use a combination of vision and audio encoders fused into a text generator to test these relationships.üõ†Ô∏è Model ArchitectureThe system integrates three state-of-the-art models:Vision Encoder: VideoMAE ‚Äì Processes video frames to extract spatial-temporal features.Audio Encoder: AST (Audio Spectrogram Transformer) ‚Äì Converts audio into frequency-based features.Text Decoder: GIT ‚Äì Fuses the multimodal embeddings to generate descriptive captions.üß™ Evaluation MethodologyWe evaluate the model using Zero-Shot Captioning and Counterfactual Swap Tests:Matched Test: Pairing a video (e.g., breaking glass) with its original audio.Swapped Test: Pairing the same video with conflicting audio (e.g., a bird squawk) to see if the model detects the mismatch and updates its caption.[Image showing the difference between a matched total effect and a swapped total effect]üìä Key Metrics & ResultsWe use Causal Effect metrics to measure the "strength" of the audio-visual connection. Below are the results from our initial Matched Test:MetricValueInterpretationTotal Effect (TE)0.1052Overall influence of audio on the output.Natural Direct Effect (NDE)0.1052Direct impact of audio on word choice.Natural Indirect Effect (NIE)0.0000Impact of audio mediated through visual perception.Modality Sensitivity0.8266The model's overall reliance on the audio stream.Findings:In the Swapped Test, the Total Effect dropped to -0.1725, proving the model successfully identified the audio-visual conflict.The high Modality Sensitivity confirms the model is actively "listening" rather than just guessing from the picture.
